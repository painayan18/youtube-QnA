{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.venv/lib/python3.12/site-packages (0.3.14)\n",
      "Collecting yt_dlp==2023.7.6\n",
      "  Using cached yt_dlp-2023.7.6-py2.py3-none-any.whl.metadata (157 kB)\n",
      "Collecting tiktoken==0.5.1\n",
      "  Using cached tiktoken-0.5.1.tar.gz (32 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting docarray==0.38.0\n",
      "  Using cached docarray-0.38.0-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (1.0.1)\n",
      "Collecting mutagen (from yt_dlp==2023.7.6)\n",
      "  Using cached mutagen-1.47.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting pycryptodomex (from yt_dlp==2023.7.6)\n",
      "  Using cached pycryptodomex-3.21.0-cp36-abi3-macosx_10_9_universal2.whl.metadata (3.4 kB)\n",
      "Collecting websockets (from yt_dlp==2023.7.6)\n",
      "  Using cached websockets-14.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from yt_dlp==2023.7.6) (2024.12.14)\n",
      "Collecting brotli (from yt_dlp==2023.7.6)\n",
      "  Using cached Brotli-1.1.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.12/site-packages (from tiktoken==0.5.1) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.venv/lib/python3.12/site-packages (from tiktoken==0.5.1) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./.venv/lib/python3.12/site-packages (from docarray==0.38.0) (2.2.1)\n",
      "Requirement already satisfied: orjson>=3.8.2 in ./.venv/lib/python3.12/site-packages (from docarray==0.38.0) (3.10.13)\n",
      "Collecting pydantic<2.0.0,>=1.10.2 (from docarray==0.38.0)\n",
      "  Using cached pydantic-1.10.20-cp312-cp312-macosx_11_0_arm64.whl.metadata (153 kB)\n",
      "Requirement already satisfied: rich>=13.1.0 in ./.venv/lib/python3.12/site-packages (from docarray==0.38.0) (13.9.4)\n",
      "Requirement already satisfied: types-requests>=2.28.11.6 in ./.venv/lib/python3.12/site-packages (from docarray==0.38.0) (2.32.0.20241016)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.12/site-packages (from docarray==0.38.0) (0.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.12/site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.12/site-packages (from langchain) (3.11.11)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in ./.venv/lib/python3.12/site-packages (from langchain) (0.3.29)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in ./.venv/lib/python3.12/site-packages (from langchain) (0.3.4)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in ./.venv/lib/python3.12/site-packages (from langchain) (0.2.10)\n",
      "INFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain-0.3.12-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.10-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.9-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.8-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting numpy>=1.17.3 (from docarray==0.38.0)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain-0.3.6-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.5-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.2.17-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.43 (from langchain)\n",
      "  Downloading langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.43->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.43->langchain) (24.2)\n",
      "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.38 (from langchain)\n",
      "  Downloading langchain_core-0.2.42-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.41-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.40-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.39-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.38-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.15-py3-none-any.whl.metadata (7.1 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting langchain-core<0.3.0,>=0.2.35 (from langchain)\n",
      "  Downloading langchain_core-0.2.37-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.36-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.35-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.14-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.32 (from langchain)\n",
      "  Downloading langchain_core-0.2.34-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.33-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.32-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.13-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.30 (from langchain)\n",
      "  Downloading langchain_core-0.2.30-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.12-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.27 (from langchain)\n",
      "  Downloading langchain_core-0.2.29-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.28-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.27-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.11-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.23 (from langchain)\n",
      "  Downloading langchain_core-0.2.26-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.25-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.24-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading langchain_core-0.2.23-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.10-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.22 (from langchain)\n",
      "  Downloading langchain_core-0.2.22-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.9-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.20 (from langchain)\n",
      "  Downloading langchain_core-0.2.21-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.2.20-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.8-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.19 (from langchain)\n",
      "  Downloading langchain_core-0.2.19-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.7-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.12 (from langchain)\n",
      "  Downloading langchain_core-0.2.18-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.2.17-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.2.15-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.2.13-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.2.12-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.10 (from langchain)\n",
      "  Downloading langchain_core-0.2.11-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.2.10-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.5-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.7 (from langchain)\n",
      "  Downloading langchain_core-0.2.9-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.2.8-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./.venv/lib/python3.12/site-packages (from pydantic<2.0.0,>=1.10.2->docarray==0.38.0) (4.12.2)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.28.1)\n",
      "INFO: pip is looking at multiple versions of langsmith to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.146-py3-none-any.whl.metadata (14 kB)\n",
      "  Downloading langsmith-0.1.145-py3-none-any.whl.metadata (14 kB)\n",
      "  Downloading langsmith-0.1.144-py3-none-any.whl.metadata (14 kB)\n",
      "  Downloading langsmith-0.1.143-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.142-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.139-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.138-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of langsmith to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langsmith-0.1.137-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.134-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.133-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.132-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.131-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading langsmith-0.1.129-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.128-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.127-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.126-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.125-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.124-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.123-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.122-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.121-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.120-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.119-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.118-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.117-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.116-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.115-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.114-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.113-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.112-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.111-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.110-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.109-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.108-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.107-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.106-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.105-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.104-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.103-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.102-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.101-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.99-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.98-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.97-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.96-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.95-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.94-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.93-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.92-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.91-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.90-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.89-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.88-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.87-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.86-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.85-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.84-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.83-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.82-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading langsmith-0.1.81-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.1) (2.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich>=13.1.0->docarray==0.38.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich>=13.1.0->docarray==0.38.0) (2.19.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->docarray==0.38.0) (1.0.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.43->langchain) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray==0.38.0) (0.1.2)\n",
      "Using cached yt_dlp-2023.7.6-py2.py3-none-any.whl (3.0 MB)\n",
      "Using cached docarray-0.38.0-py3-none-any.whl (264 kB)\n",
      "Downloading langchain-0.2.5-py3-none-any.whl (974 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m491.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.2.8-py3-none-any.whl (315 kB)\n",
      "Using cached pydantic-1.10.20-cp312-cp312-macosx_11_0_arm64.whl (2.5 MB)\n",
      "Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
      "Downloading langsmith-0.1.81-py3-none-any.whl (127 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached Brotli-1.1.0-cp312-cp312-macosx_10_13_universal2.whl (815 kB)\n",
      "Using cached mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
      "Using cached pycryptodomex-3.21.0-cp36-abi3-macosx_10_9_universal2.whl (2.5 MB)\n",
      "Using cached websockets-14.1-cp312-cp312-macosx_11_0_arm64.whl (159 kB)\n",
      "Building wheels for collected packages: tiktoken\n",
      "  Building wheel for tiktoken (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tiktoken \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[36 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.13-universal2-cpython-312/tiktoken\n",
      "  \u001b[31m   \u001b[0m copying tiktoken/registry.py -> build/lib.macosx-10.13-universal2-cpython-312/tiktoken\n",
      "  \u001b[31m   \u001b[0m copying tiktoken/__init__.py -> build/lib.macosx-10.13-universal2-cpython-312/tiktoken\n",
      "  \u001b[31m   \u001b[0m copying tiktoken/core.py -> build/lib.macosx-10.13-universal2-cpython-312/tiktoken\n",
      "  \u001b[31m   \u001b[0m copying tiktoken/model.py -> build/lib.macosx-10.13-universal2-cpython-312/tiktoken\n",
      "  \u001b[31m   \u001b[0m copying tiktoken/load.py -> build/lib.macosx-10.13-universal2-cpython-312/tiktoken\n",
      "  \u001b[31m   \u001b[0m copying tiktoken/_educational.py -> build/lib.macosx-10.13-universal2-cpython-312/tiktoken\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.13-universal2-cpython-312/tiktoken_ext\n",
      "  \u001b[31m   \u001b[0m copying tiktoken_ext/openai_public.py -> build/lib.macosx-10.13-universal2-cpython-312/tiktoken_ext\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing tiktoken.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to tiktoken.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to tiktoken.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to tiktoken.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'tiktoken.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching 'Makefile'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'tiktoken.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m copying tiktoken/py.typed -> build/lib.macosx-10.13-universal2-cpython-312/tiktoken\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m running build_rust\n",
      "  \u001b[31m   \u001b[0m error: can't find Rust compiler\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m To update pip, run:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     pip install --upgrade pip\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m and then retry package installation.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for tiktoken\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build tiktoken\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tiktoken)\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain-community yt_dlp==2023.7.6 tiktoken==0.5.1 docarray==0.38.0 python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from openai import OpenAI\n",
    "import yt_dlp as youtube_dl\n",
    "from yt_dlp import DownloadError\n",
    "import docarray\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = api_key\n",
    "\n",
    "youtube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n",
    "\n",
    "output_dir = \"files/audio\"\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# configure youtube-dl\n",
    "ydl_config = {\n",
    "    \"format\": \"bestaudio/best\",\n",
    "    \"postprocessors\": [\n",
    "        {\n",
    "            \"key\": \"FFmpegExtractAudio\",\n",
    "            \"preferredcodec\": \"mp3\",\n",
    "            \"preferredquality\": \"192\"\n",
    "        }\n",
    "    ],\n",
    "    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Encodings: locale UTF-8, fs utf-8, pref UTF-8, out UTF-8 (No ANSI), error UTF-8 (No ANSI), screen UTF-8 (No ANSI)\n",
      "[debug] yt-dlp version stable@2024.12.23 from yt-dlp/yt-dlp [65cf46cdd] (pip) API\n",
      "[debug] params: {'format': 'bestaudio/best', 'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}], 'outtmpl': 'files/audio/%(title)s.%(ext)s', 'verbose': True, 'compat_opts': set(), 'http_headers': {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.55 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en-us,en;q=0.5', 'Sec-Fetch-Mode': 'navigate'}}\n",
      "[debug] Python 3.12.8 (CPython arm64 64bit) - macOS-15.2-arm64-arm-64bit (OpenSSL 3.0.15 3 Sep 2024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading video from https://www.youtube.com/watch?v=aqzxYofJ_ck\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] exe versions: ffmpeg 7.1 (setts), ffprobe 7.1\n",
      "[debug] Optional libraries: certifi-2024.12.14, requests-2.32.3, sqlite3-3.45.3, urllib3-2.3.0\n",
      "[debug] Proxy map: {}\n",
      "[debug] Request Handlers: urllib, requests\n",
      "[debug] Loaded 1837 extractors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=aqzxYofJ_ck\n",
      "[youtube] aqzxYofJ_ck: Downloading webpage\n",
      "[youtube] aqzxYofJ_ck: Downloading ios player API JSON\n",
      "[youtube] aqzxYofJ_ck: Downloading mweb player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] [youtube] aqzxYofJ_ck: ios client https formats require a PO Token which was not provided. They will be skipped as they may yield HTTP Error 403. You can manually pass a PO Token for this client with --extractor-args \"youtube:po_token=ios+XXX. For more information, refer to  https://github.com/yt-dlp/yt-dlp/wiki/Extractors#po-token-guide . To enable these broken formats anyway, pass --extractor-args \"youtube:formats=missing_pot\"\n",
      "[debug] Loading youtube-nsig.3ede36f2 from cache\n",
      "[debug] [youtube] Decrypted nsig L8wFS-4spegWlKrZhx => RurmDxeSfUoMqA\n",
      "[debug] Loading youtube-nsig.3ede36f2 from cache\n",
      "[debug] [youtube] Decrypted nsig EcAsm_Jrr90Gc57aUa => 2cSF1wfWnmqOGQ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] aqzxYofJ_ck: Downloading m3u8 information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Sort order given by extractor: quality, res, fps, hdr:12, source, vcodec, channels, acodec, lang, proto\n",
      "[debug] Formats sorted by: hasvid, ie_pref, quality, res, fps, hdr:12(7), source, vcodec, channels, acodec, lang, proto, size, br, asr, vext, aext, hasaud, id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] aqzxYofJ_ck: Downloading 1 format(s): 251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] Invoking http downloader on \"https://rr1---sn-nx5s7n76.googlevideo.com/videoplayback?expire=1736807833&ei=OUGFZ76_JozasfIPnYeruQg&ip=149.40.62.19&id=o-AHwwvto5DNGoCN_wd_t389Yb6Gnp9e8QEQNv4H4kc3hN&itag=251&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1736786233%2C&mh=zw&mm=31%2C26&mn=sn-nx5s7n76%2Csn-o097znss&ms=au%2Conr&mv=m&mvi=1&pl=24&rms=au%2Cau&initcwndbps=1375000&bui=AY2Et-P0TKKBnp54JBnvO2_nTjVEe5KrMU-g7RxYNb7eZ4T-dLDcYO6tR7xm2D2QZ1tkjZimCz30aggG&vprv=1&svpuc=1&mime=audio%2Fwebm&ns=hWNXj5qd6fNclsOhwhzD0MwQ&rqh=1&gir=yes&clen=10932652&dur=752.701&lmt=1654008313150389&mt=1736785753&fvip=5&keepalive=yes&fexp=51326932%2C51331020%2C51335594%2C51353498%2C51371294&c=MWEB&sefc=1&txp=5318224&n=2cSF1wfWnmqOGQ&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRQIgeZJlPJsf-l03oancDvvKhNPKc3Cn0-oMyNpX6UbHTskCIQDdLumkWXEpXE4d6drY3xuba0tcOXDW1GzFIs1zpAjqxw%3D%3D&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Crms%2Cinitcwndbps&lsig=AGluJ3MwRQIgAjJLx27UbTJarwOuA4VyO1eUkxrdaxsMAl-bCKLfv3sCIQCPhkKhXfmCexi2SQv9Y4xSBj9ZrSgPQaAy2vOI7pZxiw%3D%3D\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] Destination: files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm\n",
      "[download] 100% of   10.43MiB in 00:00:04 at 2.41MiB/s     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] ffmpeg command line: ffprobe -show_streams 'file:files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ExtractAudio] Destination: files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[debug] ffmpeg command line: ffmpeg -y -loglevel repeat+info -i 'file:files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm' -vn -acodec libmp3lame -b:a 192.0k -movflags +faststart 'file:files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting original file files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.webm (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "# check if output_dir exists, if not create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "   \n",
    "# output download status \n",
    "print(f\"Downloading video from {youtube_url}\")\n",
    "\n",
    "# attempt download using specified config\n",
    "# if DownloadError occurs, try again\n",
    "try:\n",
    "    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n",
    "        ydl.download([youtube_url])\n",
    "except DownloadError:\n",
    "    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n",
    "        ydl.download([youtube_url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files/audio/Python Machine Learning Tutorial ｜ Splitting Your Data ｜ Databytes.mp3\n"
     ]
    }
   ],
   "source": [
    "# find audio file(s) in the output directory\n",
    "audio_files = glob.glob(os.path.join(output_dir, \"*.mp3\"))\n",
    "\n",
    "# select first audio file\n",
    "audio_filename = audio_files[0]\n",
    "\n",
    "# print the name of the file\n",
    "print(audio_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transcribe Audio using Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting audio ot text... \n"
     ]
    }
   ],
   "source": [
    "audio_file = audio_filename\n",
    "output_file = \"files/transcripts/transcript.txt\"\n",
    "model = \"whisper-1\"\n",
    "\n",
    "print(\"converting audio ot text... \")\n",
    "\n",
    "with open(audio_file, \"rb\") as audio:\n",
    "    response = client.audio.transcriptions.create(\n",
    "        model='whisper-1', \n",
    "        file = audio\n",
    "        )\n",
    "    \n",
    "transcript = (response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save transcript to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, in this tutorial, we're going to look at a data pre-processing technique for machine learning called splitting your data. That is splitting your data set into a training set and a testing set. Now, before we get to the code, you might wonder, why do I need to do this? And really, there are going to be two problems if you don't. So if you train your machine learning model on your whole data set, then you've not tested the model on anything else. And that means you don't know how well your model is going to perform on other data sets. Secondly, it's actually even worse than this, because you risk overfitting the model. And that means that you've made your model work really well for one data set, but that gives a cost of model performance on other data sets. So not only do you not know how well the model is going to perform on other data sets, it's probably going to be worse than it could be. So you might also wonder when in your machine learning workflow, as you're writing these different types of code, when does this come? So what's the point when you need to split your data set? And it's normally the last thing you do before feature engineering. So if you do this after feature engineering, then you risk having a problem called data leakage. And that means information from the testing set is going to be available in the training set, which is a form of cheating because it's going to make your model appear to perform better than it actually does. So it's giving you a sort of false sense of security. So if you find yourself doing feature engineering and you've not yet split your data into training and testing sets, then you need to back up a step. We're going to take a look at some loan application data. So I'm using Datacamp Workspace here. And this is one of the data sets that is available as standard with Datacamp Workspace. So there is a workspace template available if you want to try doing your own analysis on this data set. So because this data is in CSV format, I'm going to import the pandas package as pd. That's the sort of standard alias for it. And then we actually we purchased one function from scikit-learn. So this is in the scikit-learn model selection submodule. And the function for splitting into training and testing sets is called train-test-split. So let's run that. All right, so this data is about loan applications. So I'm just going to call it loan-applications. And we can use pd.read.csv because it is in a CSV file. And the file is called loan-data.csv. Let me just check and see if I got that correct. loan-data.csv. Yes, it did. Okay, so let me just copy and paste this variable name so we can print out the results. Okay, so here you can see the table here. Actually, to make this easier, we've got 9,500 rows here. What I'm going to do is I'm just going to import the first 1,000 rows. And this is going to make some of the results a bit easier to understand. All right, so now we've only got 1,000 rows of data. You can see we've got this column called credit policy. This is going to be our response variable. And then we've got a load of other variables we can use as features. This purpose column, because it's a categorical column, that's going to become important how we deal with that in a moment. All right, so first of all, we'll just concentrate on the response. So the response variable is called credit.policy. And so each row is an application. So when the application meets the underwriting policy, so it meets the kind of loan criteria, it takes the value one and it's a zero if the application was not up to scratch. So I'm going to call this variable response. Some people like to call the response variable just lowercase y. I think response is a bit more meaningful, particularly in this case. So we're going to start off with the loan applications data frame. And we're going to take the credit policy column and then I'm going to copy and paste this variable name again so you can see the results. So in this case, it is a Pandas series and it's got ones and zeros. All right, so we're going to use all the other columns for features. So again, let's call this variable features. Some people like to use capital X for this. So again, I'm going to start with loan applications with a T somewhere in there. And we use every column except credit policy. So this drop method is a little shortcut for just like saying I want everything in the data frame except a specific set of columns. Now, one extra little trick. So as I mentioned, we have this categorical variable called purpose. So we need to do one hot encoding on this in order to turn it into a series of numeric columns with zeros and ones. So we can use pd.get dummies for that. Let's code another line so it's easy to see how it breaks down. And then I'm going to print this out. So, so far this is pretty standard code just for splitting your data set into a response variable and some features. So here we've got 19 columns now. So the one important thing to note is that purpose column is now several different columns with ones and zeros. All right, so now the crux of this. So we're going to split these, this, the response and the features up into training sets and testing sets. So we're going to call the train test split function and pass those two variables in. So we're going to call train test split. I'm going to pass it the response first and then the features. So let's run this and you can see the output. It's a little bit squiffy. So what we get is a list and it actually has four different things in it. So we get the responses and the features for the training set and the responses and features for the testing set. And actually the trick is just remembering which order they're returned in. So rather than returning a list, it's slightly easier if we use variable unpacking and we return four different objects together. So we're going to return four things from this one function call. So I'm going to call this response train and I think this one is a response test and then we go features train and features test. So we've got four different variables here and let me run that. So I can sort of print these out one at a time, but it's not that exciting because you've seen the whole data set before. It's just bits of it. So what's actually slightly... paste it twice. So what's actually slightly more useful is if we take a look at the shape of each of these and you can see how much data has ended up in each one. So I'm going to print this out and we're going to do the same with the test set and features test. All right, so we've got four different variables here. Now you can see we started off with a thousand rows and so the responses, these are series, they don't have any columns, but you can see that 75% of them, so 750 out of a thousand, have ended up in the training set and 250, I'll just highlight that, have ended up in the testing set. And it's the same with the features as well. So we've got 750 in the training set and 250 in the test set. You've got 19 columns here because 19 features. So by default, 75% of our data has ended up in the training set, 25% has ended up in the testing set. And normally this is perfectly fine. Sometimes if you have a small data set, you might want a little bit more in the training set and a little bit less in the testing set. And if you've got a very large data set, then you might say, well, okay, I want 70% of data in the training set and 30% in the testing set. So in this case, because we've only got a thousand rows, let's shrink the testing set a little bit. So we're going to do the same again, but we're going to use the test size argument to get a smaller testing set. So I'm just going to copy and paste this code, run the typing again. So the change here, we're going to add an additional argument called test size, and we set that to 0.2. So we're going to have 80% in the training set, 20% in the test set. Let's run that. And again, I am going to copy and paste this code that shows the shapes of the output. And here you can see now we've got 800 of the thousand rows in the training set, 200 in the testing set. Now, one more thing you might be interested in doing is reproducing the values that are provided in both the training testing sets. What I mean by that is that by default, the training testing sets are randomly generated. So each of the rows from the data set is randomly allocated to one or other of these sets. If you're writing a report, you might want to have your results exactly the same every time. Another case where this is useful is if you're trying to find a problem with your model, then you might want to be able to demonstrate the problem precisely to someone else. So sometimes you want your code to be exactly reproducible, despite the fact that you've got random things in it. And for this work, you need to set a random seed, and you can use the random state argument to do this. So what I'm going to do is I'm going to run this code twice, but we're going to set the random state argument. You can make any number you like. I'm just going to pick 999. And so because we set the random state, this code's going to run the same thing twice. Let me give different variable names for what's being returned here. So I'm going to run this. So we've done the split twice. And so let's have a look at one of these. So if you have a look at features train, you can see the values. You've got row 46, row 748, and so on. And then let's add another one of these. So we're going to look at features train two. And you see, even though it's random, we have exactly the same results. So it's row 46, row 748, row 524, and so on. So exactly the same result in both cases. And that's more or less all there is to splitting your data into training and testing sets. I hope it's been helpful. you\n"
     ]
    }
   ],
   "source": [
    "if output_file is not None:\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(transcript)\n",
    "    \n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Textloader using Langchain; create document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./files/transcripts/transcript.txt\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './files/transcripts/transcript.txt'}, page_content=\"Hi, in this tutorial, we're going to look at a data pre-processing technique for machine learning called splitting your data. That is splitting your data set into a training set and a testing set. Now, before we get to the code, you might wonder, why do I need to do this? And really, there are going to be two problems if you don't. So if you train your machine learning model on your whole data set, then you've not tested the model on anything else. And that means you don't know how well your model is going to perform on other data sets. Secondly, it's actually even worse than this, because you risk overfitting the model. And that means that you've made your model work really well for one data set, but that gives a cost of model performance on other data sets. So not only do you not know how well the model is going to perform on other data sets, it's probably going to be worse than it could be. So you might also wonder when in your machine learning workflow, as you're writing these different types of code, when does this come? So what's the point when you need to split your data set? And it's normally the last thing you do before feature engineering. So if you do this after feature engineering, then you risk having a problem called data leakage. And that means information from the testing set is going to be available in the training set, which is a form of cheating because it's going to make your model appear to perform better than it actually does. So it's giving you a sort of false sense of security. So if you find yourself doing feature engineering and you've not yet split your data into training and testing sets, then you need to back up a step. We're going to take a look at some loan application data. So I'm using Datacamp Workspace here. And this is one of the data sets that is available as standard with Datacamp Workspace. So there is a workspace template available if you want to try doing your own analysis on this data set. So because this data is in CSV format, I'm going to import the pandas package as pd. That's the sort of standard alias for it. And then we actually we purchased one function from scikit-learn. So this is in the scikit-learn model selection submodule. And the function for splitting into training and testing sets is called train-test-split. So let's run that. All right, so this data is about loan applications. So I'm just going to call it loan-applications. And we can use pd.read.csv because it is in a CSV file. And the file is called loan-data.csv. Let me just check and see if I got that correct. loan-data.csv. Yes, it did. Okay, so let me just copy and paste this variable name so we can print out the results. Okay, so here you can see the table here. Actually, to make this easier, we've got 9,500 rows here. What I'm going to do is I'm just going to import the first 1,000 rows. And this is going to make some of the results a bit easier to understand. All right, so now we've only got 1,000 rows of data. You can see we've got this column called credit policy. This is going to be our response variable. And then we've got a load of other variables we can use as features. This purpose column, because it's a categorical column, that's going to become important how we deal with that in a moment. All right, so first of all, we'll just concentrate on the response. So the response variable is called credit.policy. And so each row is an application. So when the application meets the underwriting policy, so it meets the kind of loan criteria, it takes the value one and it's a zero if the application was not up to scratch. So I'm going to call this variable response. Some people like to call the response variable just lowercase y. I think response is a bit more meaningful, particularly in this case. So we're going to start off with the loan applications data frame. And we're going to take the credit policy column and then I'm going to copy and paste this variable name again so you can see the results. So in this case, it is a Pandas series and it's got ones and zeros. All right, so we're going to use all the other columns for features. So again, let's call this variable features. Some people like to use capital X for this. So again, I'm going to start with loan applications with a T somewhere in there. And we use every column except credit policy. So this drop method is a little shortcut for just like saying I want everything in the data frame except a specific set of columns. Now, one extra little trick. So as I mentioned, we have this categorical variable called purpose. So we need to do one hot encoding on this in order to turn it into a series of numeric columns with zeros and ones. So we can use pd.get dummies for that. Let's code another line so it's easy to see how it breaks down. And then I'm going to print this out. So, so far this is pretty standard code just for splitting your data set into a response variable and some features. So here we've got 19 columns now. So the one important thing to note is that purpose column is now several different columns with ones and zeros. All right, so now the crux of this. So we're going to split these, this, the response and the features up into training sets and testing sets. So we're going to call the train test split function and pass those two variables in. So we're going to call train test split. I'm going to pass it the response first and then the features. So let's run this and you can see the output. It's a little bit squiffy. So what we get is a list and it actually has four different things in it. So we get the responses and the features for the training set and the responses and features for the testing set. And actually the trick is just remembering which order they're returned in. So rather than returning a list, it's slightly easier if we use variable unpacking and we return four different objects together. So we're going to return four things from this one function call. So I'm going to call this response train and I think this one is a response test and then we go features train and features test. So we've got four different variables here and let me run that. So I can sort of print these out one at a time, but it's not that exciting because you've seen the whole data set before. It's just bits of it. So what's actually slightly... paste it twice. So what's actually slightly more useful is if we take a look at the shape of each of these and you can see how much data has ended up in each one. So I'm going to print this out and we're going to do the same with the test set and features test. All right, so we've got four different variables here. Now you can see we started off with a thousand rows and so the responses, these are series, they don't have any columns, but you can see that 75% of them, so 750 out of a thousand, have ended up in the training set and 250, I'll just highlight that, have ended up in the testing set. And it's the same with the features as well. So we've got 750 in the training set and 250 in the test set. You've got 19 columns here because 19 features. So by default, 75% of our data has ended up in the training set, 25% has ended up in the testing set. And normally this is perfectly fine. Sometimes if you have a small data set, you might want a little bit more in the training set and a little bit less in the testing set. And if you've got a very large data set, then you might say, well, okay, I want 70% of data in the training set and 30% in the testing set. So in this case, because we've only got a thousand rows, let's shrink the testing set a little bit. So we're going to do the same again, but we're going to use the test size argument to get a smaller testing set. So I'm just going to copy and paste this code, run the typing again. So the change here, we're going to add an additional argument called test size, and we set that to 0.2. So we're going to have 80% in the training set, 20% in the test set. Let's run that. And again, I am going to copy and paste this code that shows the shapes of the output. And here you can see now we've got 800 of the thousand rows in the training set, 200 in the testing set. Now, one more thing you might be interested in doing is reproducing the values that are provided in both the training testing sets. What I mean by that is that by default, the training testing sets are randomly generated. So each of the rows from the data set is randomly allocated to one or other of these sets. If you're writing a report, you might want to have your results exactly the same every time. Another case where this is useful is if you're trying to find a problem with your model, then you might want to be able to demonstrate the problem precisely to someone else. So sometimes you want your code to be exactly reproducible, despite the fact that you've got random things in it. And for this work, you need to set a random seed, and you can use the random state argument to do this. So what I'm going to do is I'm going to run this code twice, but we're going to set the random state argument. You can make any number you like. I'm just going to pick 999. And so because we set the random state, this code's going to run the same thing twice. Let me give different variable names for what's being returned here. So I'm going to run this. So we've done the split twice. And so let's have a look at one of these. So if you have a look at features train, you can see the values. You've got row 46, row 748, and so on. And then let's add another one of these. So we're going to look at features train two. And you see, even though it's random, we have exactly the same results. So it's row 46, row 748, row 524, and so on. So exactly the same result in both cases. And that's more or less all there is to splitting your data into training and testing sets. I hope it's been helpful. you\")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Vector Store (In-Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jz/xcfv8rn547bfxwtqs2zq34780000gn/T/ipykernel_80967/3310968786.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  OpenAIEmbeddings()\n",
      "/Users/nayanpai/Documents/youtube-gtp/.venv/lib/python3.12/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "# create a new DocArrayInMemorySearch instance\n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs,\n",
    "    OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DocArrayInMemeorySearch instance to a retreiver\n",
    "retriever = db.as_retriever\n",
    "\n",
    "# Create a new ChatOpenAI instance with a temperature of 0.0\n",
    "llm = ChatOpenAI(temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for RetrievalQA\nretriever\n  Input should be a valid dictionary or instance of BaseRetriever [type=model_type, input_value=<bound method VectorStore... object at 0x119b77590>>, input_type=method]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m qa_stuff \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_chain_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstuff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/youtube-gtp/.venv/lib/python3.12/site-packages/langchain/chains/retrieval_qa/base.py:118\u001b[0m, in \u001b[0;36mBaseRetrievalQA.from_chain_type\u001b[0;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m _chain_type_kwargs \u001b[38;5;241m=\u001b[39m chain_type_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    115\u001b[0m combine_documents_chain \u001b[38;5;241m=\u001b[39m load_qa_chain(\n\u001b[1;32m    116\u001b[0m     llm, chain_type\u001b[38;5;241m=\u001b[39mchain_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_chain_type_kwargs\n\u001b[1;32m    117\u001b[0m )\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcombine_documents_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_documents_chain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/youtube-gtp/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     emit_warning()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/youtube-gtp/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     emit_warning()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/youtube-gtp/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/youtube-gtp/.venv/lib/python3.12/site-packages/pydantic/main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    221\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for RetrievalQA\nretriever\n  Input should be a valid dictionary or instance of BaseRetriever [type=model_type, input_value=<bound method VectorStore... object at 0x119b77590>>, input_type=method]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type"
     ]
    }
   ],
   "source": [
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is this video about?\"\n",
    "\n",
    "response = qa_stuff.run(query)\n",
    "\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
